# from tokenize import generate_tokens
#
# x = "this is a test string"
# y = []
# y.append(list(generate_tokens(x.readline)))
# print(y)

s = "hello world :)"
s = s.split(' ')
print(s)